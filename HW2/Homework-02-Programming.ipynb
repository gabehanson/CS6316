{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bias-Variance Tradeoff: Classification (11 points)\n",
    "\n",
    "In our lecture, we talked the bias-variance tradeoff in a regression case, where we found that a regression model is the one that can find a tradeoff between the **bias** between the expected model and the Bayes predictor and the **variance** between the expected model and the model trained on one specific training set. \n",
    "\n",
    "In this assignment, we aim to study a similar tradeoff in a classification case. Specifically, we will use the SVM model with a RBF kernel and study how the model behaves with different $\\gamma$'s. \n",
    "\n",
    "The high-level idea of this assignment is the exactly the same as the one discussed in our lecture. Please review the **bias-variance tradeoff** for more details. \n",
    "\n",
    "**Submission instruction**\n",
    "\n",
    "- Rename this file with your computingID, as [ComputingID]-hw02.ipynb\n",
    "- Please keep all the outputs in this notebook for grading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary packages\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Generation (3 points)\n",
    "\n",
    "In the `generate_data()` function, please implement the data generation process with the following requirements. \n",
    "\n",
    "For a given number of sample specified by `n_sample`, the examples are generated from the Gaussian distribution with specific mean and variance. \n",
    "\n",
    "- 50% examples are generated from the Gaussian distribution with mean as $[0,0]$ and variance as $[[1,0],[0,1]]$, and labeled as POSITIVE\n",
    "- 25% examples are generated from the Gaussian distribution with mean as $[2,2]$ and variance as $[[2,0],[0,2]]$, and labeled as NEGATIVE\n",
    "- 25% examples are generated from the Gaussian distribution with mean as $[2,-1]$ and variance as $[[0.5,0],[0,0.5]]$, and labeled as NEGATIVE\n",
    "\n",
    "For each component, you can use the function [multivariate_normal()](https://numpy.org/doc/stable/reference/random/generated/numpy.random.multivariate_normal.html) from `numpy` for sampling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data generation\n",
    "\n",
    "def generate_data(n_sample=100):\n",
    "    # ------------------------------------\n",
    "    # TODO: implement the data generation process\n",
    "    # \n",
    "    # ------------------------------------\n",
    "    return X, y\n",
    "    \n",
    "# Run the following code to test your implementation\n",
    "X_beta, y_beta = generate_data()\n",
    "plt.scatter(X_beta[:, 0], X_beta[:, 1], c=y_beta, alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Generate Test Data\n",
    "\n",
    "Computing the true error in this case is much more compliciated than the one-dimensional case as illustrated in our lectures. Therefore, we are going to use an alternative method for approximating the true error: the following code will generate a large set of examples (3K) from the data generation function that you implemented before. We are going to use the same test data for all the following evaluation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the test set, which can be used to approximate the ground-truth distribution\n",
    "X_test, y_test = generate_data(n_sample=3000)\n",
    "plt.scatter(X_test[:, 0], X_test[:, 1], c=y_test, alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Training SVMs with Generated Data (3 points)\n",
    "\n",
    "In this section, we will generate training data from the ground-truth data distribution, and train a SVM classifier with each specific training set.\n",
    "\n",
    "In the `train()` function, for each iteration $n$, please implement the components:\n",
    "\n",
    "1. Generate training data\n",
    "2. Training a model\n",
    "3. Test it on (X_test, y_test)\n",
    "4. Attach the trained classifier to the list clfs, and the accuracy number to the list accs\n",
    "\n",
    "As you can see, the function will also calcuate the mean of the variance of prediction accuracies, and return the list of classifiers `clfs` for further use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "\n",
    "def train(X_test, y_test, N=20, gamma=1.0):\n",
    "    clfs, accs = [], []\n",
    "    for n in tqdm (range (N), desc=\"Training models ...\"):\n",
    "        # -----------------------------------\n",
    "        # TODO: for each iteration n\n",
    "        # \n",
    "        # ------------------------------------\n",
    "    print(\"The mean accuracy: {} and its variance: {}\".format(np.mean(accs), np.var(accs)))\n",
    "    return clfs\n",
    "\n",
    "# Run the following line to test the function before moving forward\n",
    "train(X_test, y_test, N=2, gamma=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 The Bayes predictor\n",
    "\n",
    "Similar to the regression case, with the ground-truth data distribution, we can calculate the decision boundary given by the Bayes predictor. Although in this case, finding the analystical solution is compliciated, we are going to a numeric method for finding the decision boundary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code is ready to use\n",
    "\n",
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "class Bayes_Predictor(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def predict(self, x):\n",
    "        '''\n",
    "        x - inputs\n",
    "        '''\n",
    "        pos = multivariate_normal([0,0], [[1,0],[0,1]])\n",
    "        neg1 = multivariate_normal([2,2], [[2,0],[0,2]])\n",
    "        neg2 = multivariate_normal([2,-2], [[0.5,0],[0,0.5]])\n",
    "        pos_val = pos.pdf(x)\n",
    "        neg1_val = neg1.pdf(x)\n",
    "        neg2_val = neg2.pdf(x)\n",
    "        # print(pos_val)\n",
    "        # print(neg1_val)\n",
    "        # print(neg2_val)\n",
    "        labels = np.all([np.greater(pos_val, neg1_val), np.greater(pos_val, neg2_val)], axis=0)\n",
    "        # print(labels)\n",
    "        return labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Plot Decision Boundaries\n",
    "\n",
    "This is a function that is ready to use for drawing decision boundaries. It is **ready** to use. \n",
    "\n",
    "Note that, the first argument of `plot_decision_boundary` is a list of classification model. If you only have one classification model, then you need to put it in a list.\n",
    "\n",
    "[Reference](https://stackoverflow.com/questions/22294241/plotting-a-decision-boundary-separating-2-classes-using-matplotlibs-pyplot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code is ready to use\n",
    "\n",
    "def plot_decision_boundary(clfs, X_test, color='lightgray'):\n",
    "    '''\n",
    "    clfs - a list of classification models\n",
    "    X_test - the inputs of the TEST data \n",
    "             (it requires the inputs only for determining the range of the input space)\n",
    "    color - the color of the decision boundary\n",
    "    '''\n",
    "    h = .02  # step size in the mesh\n",
    "    x1_min, x1_max = X_test[:, 0].min() - 1, X_test[:, 0].max() + 1\n",
    "    x2_min, x2_max = X_test[:, 1].min() - 1, X_test[:, 1].max() + 1\n",
    "    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, h), np.arange(x2_min, x2_max, h))\n",
    "\n",
    "    for n in tqdm(range(len(clfs)), desc=\"Plot decision boundaries ...\"):\n",
    "        clf = clfs[n]\n",
    "        Z = clf.predict(np.c_[xx1.ravel(), xx2.ravel()])\n",
    "        Z = Z.reshape(xx1.shape)\n",
    "        plt.contour(xx1, xx2, Z, colors=color)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Put All Together (5 points)\n",
    "\n",
    "The following code will put all functions together to draw "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Training SVMs with $\\gamma=0.01$ (2 points)\n",
    "\n",
    "With $\\gamma=0.01$, we are going to visualize how the decision boundaries look like with different copies of training data. \n",
    "\n",
    "Please implement the following components with the functions that you defined before\n",
    "\n",
    "- Use `train()` to train 10 different SVM models\n",
    "- Plot the decision boundary of each SVM using `plot_decision_boundary`\n",
    "- Use the Bayes predictor defined in the `Bayes_Predictor` class and draw its decision boundary using `plot_decision_boundary`\n",
    "\n",
    "Please **keep all outputs** for grading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gamma: 0.01\n",
    "\n",
    "# The hyper-parameter of the RBF kernel\n",
    "gamma=0.01\n",
    "plt.scatter(X_test[:, 0], X_test[:, 1], c=y_test, alpha=0.5)\n",
    "\n",
    "# TODO: Training a list of SVM models and plot their decision boundaries\n",
    "# \n",
    "\n",
    "# TODO: Plot the decision boundary of the Bayes predictor\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Training models with other $\\gamma$ (1 point)\n",
    "\n",
    "Now, let's train the models with two other $\\gamma$'s using the following two code blocks\n",
    "\n",
    "- $\\gamma = 1.0$\n",
    "- $\\gamma = 10.0$\n",
    "\n",
    "You can copy and paste the code from the previous block. And keep **all outputs** for grading. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gamma: 1.0\n",
    "\n",
    "# TODO: Repeat the experiment with gamma=1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gamma: 10.0\n",
    "\n",
    "# TODO: Repeat the experiment with gamma=10.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Conclusion (2 points)\n",
    "\n",
    "What conclusion you can get from the models with different $\\gamma$'s. Please answer the question from the perspectives of \n",
    "\n",
    "- the mean prediction accuracy\n",
    "- the variance of prediction accuracy\n",
    "- the pattern of the decision boundaries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: *leave your answer here*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
