{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9684eb95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import metrics\n",
    "import time\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f49eee48",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = pd.read_csv('project3_dataset1.txt', sep = '\\t', header = None)\n",
    "data2 = pd.read_csv('project3_dataset2.txt', sep = '\\t', header = None)\n",
    "# column 4 needs to be converted to numerical 1 if present, 0 if absent\n",
    "data2[4] = data2[4].map({'Present': 1, 'Absent': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4a333c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1_X = data1.iloc[:,:30]\n",
    "data1_y = data1.iloc[:,30]\n",
    "\n",
    "data2_X = data2.iloc[:,:9]\n",
    "data2_y = data2.iloc[:,9]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4617923f",
   "metadata": {},
   "source": [
    "## dataset 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa6c1137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.41272917e+01 1.92896485e+01 9.19690334e+01 6.54889104e+02\n",
      " 9.63602812e-02 1.04340984e-01 8.87993158e-02 4.89191459e-02\n",
      " 1.81161863e-01 6.27976098e-02 4.05172056e-01 1.21685343e+00\n",
      " 2.86605923e+00 4.03370791e+01 7.04097891e-03 2.54781388e-02\n",
      " 3.18937163e-02 1.17961371e-02 2.05422988e-02 3.79490387e-03\n",
      " 1.62691898e+01 2.56772232e+01 1.07261213e+02 8.80583128e+02\n",
      " 1.32368594e-01 2.54265044e-01 2.72188483e-01 1.14606223e-01\n",
      " 2.90075571e-01 8.39458172e-02]\n"
     ]
    }
   ],
   "source": [
    "# call the method and then compute the mean and std\n",
    "scaler = preprocessing.StandardScaler()\n",
    "scaler.fit(data1_X)\n",
    "print(scaler.mean_)\n",
    "# https://scikit-learn.org/stable/modules/preprocessing.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3311502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:  [-1.62533529e-16  3.70724736e-17  4.20870134e-16  9.20958291e-17\n",
      "  1.66460284e-16 -8.48764527e-17  3.59017639e-17 -5.01453985e-17\n",
      "  1.55606830e-16 -1.49333777e-15  6.85840761e-17 -1.19997743e-16\n",
      " -8.00960548e-17 -1.65557862e-16 -1.78240551e-16  2.00831589e-16\n",
      "  1.62728647e-16 -4.48772049e-18  8.74129903e-17 -1.57131191e-17\n",
      " -5.34624093e-17 -2.88287262e-17  2.65360864e-17  2.10727745e-16\n",
      " -2.32385874e-16  5.26819361e-18  7.80473128e-17 -2.27312799e-16\n",
      "  2.71604649e-16  1.73411373e-16]\n",
      "Standard Deviation:  1.0\n"
     ]
    }
   ],
   "source": [
    "# this scales the data to have a mean of zero and a std of 1 for each feature\n",
    "X1_scaled = scaler.transform(data1_X)\n",
    "print('Mean: ', sum(X1_scaled)/len(X1_scaled))\n",
    "print('Standard Deviation: ', np.std(X1_scaled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9954273",
   "metadata": {},
   "outputs": [],
   "source": [
    "X1_train, X1_test, y1_train, y1_test = train_test_split(X1_scaled, data1_y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03b415e",
   "metadata": {},
   "source": [
    "### linear SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "60b1b6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "C_values = [100, 50, 10, 5, 1, 0.5, 0.01, 0.005, 0.001]\n",
    "\n",
    "# gamma_values\n",
    "\n",
    "params = [{'C' : C_values}]\n",
    "scoring = { \"accuracy\": make_scorer(accuracy_score), \"precision\": \"precision\", \n",
    "           \"recall\": \"recall\", \"f1\": \"f1\", \"roc_auc\": \"roc_auc\"}\n",
    "\n",
    "scoring_metrics = ['accuracy', 'precision', 'recall','f1','roc_auc']\n",
    "\n",
    "svm_clf_linear = SVC(kernel = 'linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d136c339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 9 candidates, totalling 90 fits\n",
      "The best parameters found are:  {'C': 1}\n",
      "The optimal parameters were found in : 1.2005069255828857  seconds\n",
      "The best score was : 0.9757487922705315\n"
     ]
    }
   ],
   "source": [
    "t = time.time()\n",
    "\n",
    "gs_svm_linear = GridSearchCV(estimator = svm_clf_linear,\n",
    "                      param_grid=params,\n",
    "                      scoring=scoring_metrics,\n",
    "                      cv=10,\n",
    "                        verbose = 1,\n",
    "                        refit=\"accuracy\",\n",
    "                        return_train_score=True)\n",
    "\n",
    "gs_svm_linear.fit(X1_train, y1_train)\n",
    "\n",
    "elapsed = time.time() - t\n",
    "print('The best parameters found are: ', gs_svm_linear.best_params_)\n",
    "print('The optimal parameters were found in :', elapsed, ' seconds')\n",
    "print('The best score was :', gs_svm_linear.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ca29538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bias variance figure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415408c7",
   "metadata": {},
   "source": [
    "### polynomial SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "999472fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "C_values = [100, 50, 10, 5, 1, 0.5, 0.01, 0.005, 0.001]\n",
    "degrees = [2,3,4,5,6,7]\n",
    "gamma_values = [100, 50, 10, 5, 1, 0.5, 0.01, 0.005, 0.001]\n",
    "\n",
    "params = [{'C' : C_values,\n",
    "         'degree': degrees,\n",
    "         'gamma': gamma_values}]\n",
    "\n",
    "scoring = { \"accuracy\": make_scorer(accuracy_score), \"precision\": \"precision\", \n",
    "           \"recall\": \"recall\", \"f1\": \"f1\", \"roc_auc\": \"roc_auc\"}\n",
    "\n",
    "scoring_metrics = ['accuracy', 'precision', 'recall','f1','roc_auc']\n",
    "\n",
    "svm_clf_poly = SVC(kernel = 'poly')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "787eea5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# there is a bunch of warnings that get thrown here for undefined metrics - for certain parameter sets the model never predicts these \n",
    "# sets so the precision is undefined. I acknowledge this, for the best fitting models this isn't a problem - and silence these warnings\n",
    "from warnings import simplefilter\n",
    "from sklearn.exceptions import UndefinedMetricWarning\n",
    "simplefilter(\"ignore\", category=UndefinedMetricWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a6805c0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 486 candidates, totalling 4860 fits\n",
      "The best parameters found are:  {'C': 0.01, 'degree': 3, 'gamma': 0.5}\n",
      "The optimal parameters were found in : 87.58341002464294  seconds\n"
     ]
    }
   ],
   "source": [
    "t = time.time()\n",
    "\n",
    "gs_svm_poly = GridSearchCV(estimator = svm_clf_poly,\n",
    "                      param_grid=params,\n",
    "                      scoring=scoring_metrics,\n",
    "                      cv=10,\n",
    "                        verbose = 1,\n",
    "                        refit=\"accuracy\",\n",
    "                        return_train_score=True)\n",
    "\n",
    "gs_svm_poly.fit(X1_train, y1_train)\n",
    "\n",
    "elapsed = time.time() - t\n",
    "print('The best parameters found are: ', gs_svm_poly.best_params_)\n",
    "print('The optimal parameters were found in :', elapsed, ' seconds')\n",
    "print('The best score was :', gs_svm_poly.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c1fd42d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "simplefilter(\"always\", category=UndefinedMetricWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f4ac2967",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bias variance figure "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e877c613",
   "metadata": {},
   "source": [
    "### rbf kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f10a38f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "C_values = [100, 50, 10, 5, 1, 0.5, 0.01, 0.005, 0.001]\n",
    "gamma_values = [100, 50, 10, 5, 1, 0.5, 0.01, 0.005, 0.001]\n",
    "\n",
    "params = [{'C' : C_values,\n",
    "         'gamma': gamma_values}]\n",
    "\n",
    "scoring = { \"accuracy\": make_scorer(accuracy_score), \"precision\": \"precision\", \n",
    "           \"recall\": \"recall\", \"f1\": \"f1\", \"roc_auc\": \"roc_auc\"}\n",
    "\n",
    "scoring_metrics = ['accuracy', 'precision', 'recall','f1','roc_auc']\n",
    "\n",
    "svm_clf_rbf= SVC(kernel = 'rbf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2c19f93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# there is a bunch of warnings that get thrown here for undefined metrics - for certain parameter sets the model never predicts these \n",
    "# sets so the precision is undefined. I acknowledge this, for the best fitting models this isn't a problem - and silence these warnings\n",
    "from warnings import simplefilter\n",
    "from sklearn.exceptions import UndefinedMetricWarning\n",
    "simplefilter(\"ignore\", category=UndefinedMetricWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fadd53fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 81 candidates, totalling 810 fits\n",
      "The best parameters found are:  {'C': 5, 'gamma': 0.01}\n",
      "The optimal parameters were found in : 34.40455389022827  seconds\n",
      "The best score was : 0.980144927536232\n"
     ]
    }
   ],
   "source": [
    "t = time.time()\n",
    "\n",
    "gs_svm_rbf = GridSearchCV(estimator = svm_clf_rbf,\n",
    "                      param_grid=params,\n",
    "                      scoring=scoring_metrics,\n",
    "                      cv=10,\n",
    "                        verbose = 1,\n",
    "                        refit=\"accuracy\",\n",
    "                        return_train_score=True)\n",
    "\n",
    "gs_svm_rbf.fit(X1_train, y1_train)\n",
    "\n",
    "elapsed = time.time() - t\n",
    "print('The best parameters found are: ', gs_svm_rbf.best_params_)\n",
    "print('The optimal parameters were found in :', elapsed, ' seconds')\n",
    "print('The best score was :', gs_svm_rbf.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8afa95a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "simplefilter(\"always\", category=UndefinedMetricWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f90acdfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bias variance tradeoff figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "60d6c170",
   "metadata": {},
   "outputs": [],
   "source": [
    "## best parameter are an rbf kernel, C = 5 and gamma = 0.01\n",
    "## Retrain with these parameters and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "316aa2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = SVC(kernel = 'rbf', C = 5, gamma = 0.01)\n",
    "final_model.fit(X1_train, y1_train)\n",
    "train_scores = cross_validate(final_model, X1_train, y1_train, cv=10, scoring=scoring_metrics)\n",
    "predictions1 = final_model.predict(X1_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f0e56ca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            Accuracy    Precision    Recall        F1       AUC\n",
      "----------------------------------------  ----------  -----------  --------  --------  --------\n",
      "Mean score of 10 fold CV on training set    0.980145     0.988194  0.957353  0.971457  0.993645\n",
      "Test Scores                                 0.973684     0.977778  0.956522  0.967033  0.970908\n"
     ]
    }
   ],
   "source": [
    "data = [[\"Mean score of 10 fold CV on training set\\n\", np.mean(train_scores['test_accuracy']), np.mean(train_scores['test_precision']), np.mean(train_scores['test_recall']),\n",
    "         np.mean(train_scores['test_f1']), np.mean(train_scores['test_roc_auc'])], \n",
    "         [\"Test Scores\", sklearn.metrics.accuracy_score(y1_test, predictions1), sklearn.metrics.precision_score(y1_test, predictions1), \n",
    "        sklearn.metrics.recall_score(y1_test, predictions1), sklearn.metrics.f1_score(y1_test, predictions1),\n",
    "          sklearn.metrics.roc_auc_score(y1_test, predictions1)]]\n",
    "print(tabulate(data, headers=[\"Accuracy\", \"Precision\", \"Recall\", \"F1\", \"AUC\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3184f3f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric       Mean score of 10 fold cross validation on training set    Test Scores\n",
      "---------  --------------------------------------------------------  -------------\n",
      "Accuracy                                                   0.980145       0.973684\n",
      "Precision                                                  0.988194       0.977778\n",
      "Recall                                                     0.957353       0.956522\n",
      "F1                                                         0.971457       0.967033\n",
      "AUC                                                        0.993645       0.970908\n"
     ]
    }
   ],
   "source": [
    "data = [['Accuracy', np.mean(train_scores['test_accuracy']), sklearn.metrics.accuracy_score(y1_test, predictions1)],\n",
    "['Precision', np.mean(train_scores['test_precision']),sklearn.metrics.precision_score(y1_test, predictions1)],\n",
    "['Recall', np.mean(train_scores['test_recall']), sklearn.metrics.recall_score(y1_test, predictions1)],\n",
    "['F1', np.mean(train_scores['test_f1']), sklearn.metrics.f1_score(y1_test, predictions1)],\n",
    "['AUC', np.mean(train_scores['test_roc_auc']), sklearn.metrics.roc_auc_score(y1_test, predictions1)]]\n",
    "print (tabulate(data, headers=[\"metric\", \"Mean score of 10 fold cross validation on training set\", \"Test Scores\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6efa2c0f",
   "metadata": {},
   "source": [
    "## dataset2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "145e04b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[138.32683983   3.63564935   4.74032468  25.4067316    0.41558442\n",
      "  53.1038961   26.04411255  17.04439394  42.81601732]\n"
     ]
    }
   ],
   "source": [
    "# call the method and then compute the mean and std\n",
    "scaler = preprocessing.StandardScaler()\n",
    "scaler.fit(data2_X)\n",
    "print(scaler.mean_)\n",
    "# https://scikit-learn.org/stable/modules/preprocessing.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88cdc1bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:  [-2.04742428e-16 -6.87280920e-17  3.47004772e-16  1.87440251e-16\n",
      "  1.13425383e-16 -1.10541686e-16 -2.27331381e-16  4.13630169e-17\n",
      "  1.44184808e-18]\n",
      "Standard Deviation:  1.0\n"
     ]
    }
   ],
   "source": [
    "# this scales the data to have a mean of zero and a std of 1 for each feature\n",
    "X2_scaled = scaler.transform(data2_X)\n",
    "print('Mean: ', sum(X2_scaled)/len(X2_scaled))\n",
    "print('Standard Deviation: ', np.std(X2_scaled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "540d0507",
   "metadata": {},
   "outputs": [],
   "source": [
    "X2_train, X2_test, y2_train, y2_test = train_test_split(X2_scaled, data2_y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc44c31",
   "metadata": {},
   "source": [
    "### linear svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "636066a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "C_values = [100, 50, 10, 5, 1, 0.5, 0.1, 0.05, 0.01, 0.005, 0.001]\n",
    "\n",
    "# gamma_values\n",
    "\n",
    "params = [{'C' : C_values}]\n",
    "scoring = { \"accuracy\": make_scorer(accuracy_score), \"precision\": \"precision\", \n",
    "           \"recall\": \"recall\", \"f1\": \"f1\", \"roc_auc\": \"roc_auc\"}\n",
    "\n",
    "scoring_metrics = ['accuracy', 'precision', 'recall','f1','roc_auc']\n",
    "\n",
    "svm_clf_linear = SVC(kernel = 'linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3eed3461",
   "metadata": {},
   "outputs": [],
   "source": [
    "# there is a bunch of warnings that get thrown here for undefined metrics - for certain parameter sets the model never predicts these \n",
    "# sets so the precision is undefined. I acknowledge this, for the best fitting models this isn't a problem - and silence these warnings\n",
    "from warnings import simplefilter\n",
    "from sklearn.exceptions import UndefinedMetricWarning\n",
    "simplefilter(\"ignore\", category=UndefinedMetricWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "81d1ea76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 11 candidates, totalling 110 fits\n",
      "The best parameters found are:  {'C': 0.5}\n",
      "The optimal parameters were found in : 2.6282360553741455  seconds\n",
      "The best score was : 0.728978978978979\n"
     ]
    }
   ],
   "source": [
    "t = time.time()\n",
    "\n",
    "gs_svm_linear = GridSearchCV(estimator = svm_clf_linear,\n",
    "                      param_grid=params,\n",
    "                      scoring=scoring_metrics,\n",
    "                      cv=10,\n",
    "                        verbose = 1,\n",
    "                        refit=\"accuracy\",\n",
    "                        return_train_score=True)\n",
    "\n",
    "gs_svm_linear.fit(X2_train, y2_train)\n",
    "\n",
    "elapsed = time.time() - t\n",
    "print('The best parameters found are: ', gs_svm_linear.best_params_)\n",
    "print('The optimal parameters were found in :', elapsed, ' seconds')\n",
    "print('The best score was :', gs_svm_linear.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd5de111",
   "metadata": {},
   "outputs": [],
   "source": [
    "simplefilter(\"always\", category=UndefinedMetricWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "493e5212",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bias variance figure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ddb89d8",
   "metadata": {},
   "source": [
    "### polynomial svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "86ce93a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "C_values = [5, 1, 0.5, 0.01, 0.05]\n",
    "degrees = [2,3,4,5]\n",
    "gamma_values = [1, 0.5, 0.01, 0.05]\n",
    "\n",
    "params = [{'C' : C_values,\n",
    "         'degree': degrees,\n",
    "         'gamma': gamma_values}]\n",
    "\n",
    "scoring = { \"accuracy\": make_scorer(accuracy_score), \"precision\": \"precision\", \n",
    "           \"recall\": \"recall\", \"f1\": \"f1\", \"roc_auc\": \"roc_auc\"}\n",
    "\n",
    "scoring_metrics = ['accuracy', 'precision', 'recall','f1','roc_auc']\n",
    "\n",
    "svm_clf_poly = SVC(kernel = 'poly')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1cdf869f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# there is a bunch of warnings that get thrown here for undefined metrics - for certain parameter sets the model never predicts these \n",
    "# sets so the precision is undefined. I acknowledge this, for the best fitting models this isn't a problem - and silence these warnings\n",
    "from warnings import simplefilter\n",
    "from sklearn.exceptions import UndefinedMetricWarning\n",
    "simplefilter(\"ignore\", category=UndefinedMetricWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4a906511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 80 candidates, totalling 800 fits\n",
      "The best parameters found are:  {'C': 0.01, 'degree': 3, 'gamma': 0.5}\n",
      "The optimal parameters were found in : 16.110707998275757  seconds\n",
      "The best score was : 0.7073573573573573\n"
     ]
    }
   ],
   "source": [
    "# this takes wayyy longer to run, so ill reduce the parameter space\n",
    "t = time.time()\n",
    "\n",
    "gs_svm_poly = GridSearchCV(estimator = svm_clf_poly,\n",
    "                      param_grid=params,\n",
    "                      scoring=scoring_metrics,\n",
    "                      cv=10,\n",
    "                        verbose = 1,\n",
    "                        refit=\"accuracy\",\n",
    "                        return_train_score=True)\n",
    "\n",
    "gs_svm_poly.fit(X2_train, y2_train)\n",
    "\n",
    "elapsed = time.time() - t\n",
    "print('The best parameters found are: ', gs_svm_poly.best_params_)\n",
    "print('The optimal parameters were found in :', elapsed, ' seconds')\n",
    "print('The best score was :', gs_svm_poly.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f3be2ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "simplefilter(\"always\", category=UndefinedMetricWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9842a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bias variance figure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be53c657",
   "metadata": {},
   "source": [
    "### rbf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c503d083",
   "metadata": {},
   "outputs": [],
   "source": [
    "C_values = [100, 50,20,15,10, 5, 1, 0.5, 0.1, 0.05, 0.01]\n",
    "gamma_values = [5,1, 0.5, 0.1, 0.01, 0.05]\n",
    "\n",
    "params = [{'C' : C_values,\n",
    "         'gamma': gamma_values}]\n",
    "\n",
    "scoring = { \"accuracy\": make_scorer(accuracy_score), \"precision\": \"precision\", \n",
    "           \"recall\": \"recall\", \"f1\": \"f1\", \"roc_auc\": \"roc_auc\"}\n",
    "\n",
    "scoring_metrics = ['accuracy', 'precision', 'recall','f1','roc_auc']\n",
    "\n",
    "svm_clf_rbf = SVC(kernel = 'rbf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "24a0a51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# there is a bunch of warnings that get thrown here for undefined metrics - for certain parameter sets the model never predicts these \n",
    "# sets so the precision is undefined. I acknowledge this, for the best fitting models this isn't a problem - and silence these warnings\n",
    "from warnings import simplefilter\n",
    "from sklearn.exceptions import UndefinedMetricWarning\n",
    "simplefilter(\"ignore\", category=UndefinedMetricWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "168abd29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 66 candidates, totalling 660 fits\n",
      "The best parameters found are:  {'C': 20, 'gamma': 0.01}\n",
      "The optimal parameters were found in : 18.644747972488403  seconds\n",
      "The best score was : 0.7452702702702703\n"
     ]
    }
   ],
   "source": [
    "# this takes wayyy longer to run, so ill reduce the parameter space\n",
    "t = time.time()\n",
    "\n",
    "gs_svm_rbf = GridSearchCV(estimator = svm_clf_rbf,\n",
    "                      param_grid=params,\n",
    "                      scoring=scoring_metrics,\n",
    "                      cv=10,\n",
    "                        verbose = 1,\n",
    "                        refit=\"accuracy\",\n",
    "                        return_train_score=True)\n",
    "\n",
    "gs_svm_rbf.fit(X2_train, y2_train)\n",
    "\n",
    "elapsed = time.time() - t\n",
    "print('The best parameters found are: ', gs_svm_rbf.best_params_)\n",
    "print('The optimal parameters were found in :', elapsed, ' seconds')\n",
    "print('The best score was :', gs_svm_rbf.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5627d235",
   "metadata": {},
   "outputs": [],
   "source": [
    "simplefilter(\"always\", category=UndefinedMetricWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53dd2819",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bias variance figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e7237648",
   "metadata": {},
   "outputs": [],
   "source": [
    "## based on accuracy, the best parameters are rbf with C = 20 and gamma = 0.01\n",
    "## I'll retrain on all training data and give the test accuracy now"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce76db2c",
   "metadata": {},
   "source": [
    "## final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b20f6d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = SVC(kernel = 'rbf', C = 20, gamma = 0.01)\n",
    "final_model.fit(X2_train, y2_train)\n",
    "train_scores = cross_validate(final_model, X2_train, y2_train, cv=10, scoring=scoring_metrics)\n",
    "predictions2 = final_model.predict(X2_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c8ee82d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            Accuracy    Precision    Recall        F1       AUC\n",
      "----------------------------------------  ----------  -----------  --------  --------  --------\n",
      "Mean score of 10 fold CV on training set    0.74527      0.692114  0.537912  0.600444  0.760844\n",
      "Test Scores                                 0.731183     0.518519  0.538462  0.528302  0.672216\n"
     ]
    }
   ],
   "source": [
    "data = [[\"Mean score of 10 fold CV on training set\\n\", np.mean(train_scores['test_accuracy']), np.mean(train_scores['test_precision']), np.mean(train_scores['test_recall']),\n",
    "         np.mean(train_scores['test_f1']), np.mean(train_scores['test_roc_auc'])], \n",
    "         [\"Test Scores\", sklearn.metrics.accuracy_score(y2_test, predictions2), sklearn.metrics.precision_score(y2_test, predictions2), \n",
    "        sklearn.metrics.recall_score(y2_test, predictions2), sklearn.metrics.f1_score(y2_test, predictions2),\n",
    "          sklearn.metrics.roc_auc_score(y2_test, predictions2)]]\n",
    "print(tabulate(data, headers=[\"Accuracy\", \"Precision\", \"Recall\", \"F1\", \"AUC\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "50ff63bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric       Mean score of 10 fold cross validation on training set    Test Scores\n",
      "---------  --------------------------------------------------------  -------------\n",
      "Accuracy                                                   0.74527        0.731183\n",
      "Precision                                                  0.692114       0.518519\n",
      "Recall                                                     0.537912       0.538462\n",
      "F1                                                         0.600444       0.528302\n",
      "AUC                                                        0.760844       0.672216\n"
     ]
    }
   ],
   "source": [
    "data = [['Accuracy', np.mean(train_scores['test_accuracy']), sklearn.metrics.accuracy_score(y2_test, predictions2)],\n",
    "['Precision', np.mean(train_scores['test_precision']),sklearn.metrics.precision_score(y2_test, predictions2)],\n",
    "['Recall', np.mean(train_scores['test_recall']), sklearn.metrics.recall_score(y2_test, predictions2)],\n",
    "['F1', np.mean(train_scores['test_f1']), sklearn.metrics.f1_score(y2_test, predictions2)],\n",
    "['AUC', np.mean(train_scores['test_roc_auc']), sklearn.metrics.roc_auc_score(y2_test, predictions2)]]\n",
    "print (tabulate(data, headers=[\"metric\", \"Mean score of 10 fold cross validation on training set\", \"Test Scores\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4319a291",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
